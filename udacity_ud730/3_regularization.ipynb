{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eren/tf_venv/tf_env_final/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some personnal imports\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in _notmnist.ipynb_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 2 to [0.0, 1.0, 0.0 ...], 3 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the logistic model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 24.315706\n",
      "Minibatch accuracy: 9.4%\n",
      "Validation accuracy: 12.9%\n",
      "Minibatch loss at step 500: 2.833301\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 1000: 1.698257\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 1500: 1.201640\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 2000: 0.922257\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 2500: 1.143535\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 3000: 0.848382\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.5%\n",
      "Test accuracy: 88.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 regularization introduces a new meta parameter that should be tuned. Since I do not have any idea of what should be the right value for this meta parameter, I will plot the accuracy by the meta parameter value (in a logarithmic scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8b84fb710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (logistic)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the same technique will improve the prediction of the 1-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 638.653076\n",
      "Minibatch accuracy: 7.0%\n",
      "Validation accuracy: 32.3%\n",
      "Minibatch loss at step 500: 198.827393\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 79.9%\n",
      "Minibatch loss at step 1000: 115.076401\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 1500: 68.027863\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 2000: 41.353893\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 2500: 25.434271\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 3000: 15.447993\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.3%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally something above 90%! I will also plot the final accuracy by the L2 parameter to find the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 3001\n",
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:    \n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "      # Pick an offset within the training data, which has been randomized.\n",
    "      # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "      # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "      # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "      # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "      # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : regul}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb8a47c0828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('Test accuracy by regularization (1-layer net)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of overfitting. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  logits = tf.matmul(lay1_train, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 391.790405\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 27.1%\n",
      "Minibatch loss at step 2: 1184.649414\n",
      "Minibatch accuracy: 46.1%\n",
      "Validation accuracy: 39.4%\n",
      "Minibatch loss at step 4: 127.025574\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 68.1%\n",
      "Minibatch loss at step 6: 3.690261\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 8: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 10: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 12: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 14: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 16: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 18: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 22: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 24: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 28: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 32: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 34: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 38: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 40: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 42: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 44: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 50: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 56: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 60: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 62: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 68: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 70: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 76: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 78: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 82: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 86: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 90: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Test accuracy: 75.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_bacthes = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_bacthes\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are far too much parameters and no regularization, the accuracy of the batches is 100%. The generalization capability is poor, as shown in the validation and test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes = 1024\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "  biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  logits = tf.matmul(drop1, weights2) + biases2\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay1_test, weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 461.701843\n",
      "Minibatch accuracy: 14.1%\n",
      "Validation accuracy: 26.7%\n",
      "Minibatch loss at step 2: 935.309204\n",
      "Minibatch accuracy: 53.1%\n",
      "Validation accuracy: 39.9%\n",
      "Minibatch loss at step 4: 244.283905\n",
      "Minibatch accuracy: 66.4%\n",
      "Validation accuracy: 60.7%\n",
      "Minibatch loss at step 6: 21.732370\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 67.9%\n",
      "Minibatch loss at step 8: 2.950897\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 10: 0.000021\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.5%\n",
      "Minibatch loss at step 12: 5.320959\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 14: 1.719266\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 69.7%\n",
      "Minibatch loss at step 16: 4.039982\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.6%\n",
      "Minibatch loss at step 18: 0.876716\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 20: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.2%\n",
      "Minibatch loss at step 22: 0.116656\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 69.9%\n",
      "Minibatch loss at step 24: 4.068783\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 69.8%\n",
      "Minibatch loss at step 26: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 69.3%\n",
      "Minibatch loss at step 28: 0.716843\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 30: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 32: 0.000001\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 34: 0.419838\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 36: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 38: 0.377035\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 40: 0.837478\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.4%\n",
      "Minibatch loss at step 42: 0.053217\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 44: 0.739756\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 46: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 48: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.7%\n",
      "Minibatch loss at step 50: 0.244431\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 52: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 54: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 56: 0.002310\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 58: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 71.0%\n",
      "Minibatch loss at step 60: 0.779212\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 71.1%\n",
      "Minibatch loss at step 62: 0.197547\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 64: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 66: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 68: 1.373038\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 70: 0.238034\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 72: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 74: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 76: 1.387945\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.9%\n",
      "Minibatch loss at step 78: 0.000207\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.3%\n",
      "Minibatch loss at step 80: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 82: 0.043983\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 84: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.5%\n",
      "Minibatch loss at step 86: 0.252849\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 88: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 90: 0.063453\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 92: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 94: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 96: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 98: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Minibatch loss at step 100: 0.000000\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 70.2%\n",
      "Test accuracy: 76.7%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 101\n",
    "num_batches = 3\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    offset = step % num_batches\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 2 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first conclusion is that 100% of accuracy on the minibatches is more difficult achieved or to keep. As a result, the test accuracy is improved by 6%, the final net is more capable of generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a first try with 2 layers. Note how the parameters are initialized, compared to the previous cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 100\n",
    "beta_regul = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  logits = tf.matmul(lay2_train, weights3) + biases3\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + \\\n",
    "      beta_regul * (tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay2_test, weights3) + biases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.257241\n",
      "Minibatch accuracy: 7.8%\n",
      "Validation accuracy: 36.8%\n",
      "Minibatch loss at step 500: 1.070463\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 1000: 0.798359\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 1500: 0.568089\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 2000: 0.600403\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 2500: 0.724441\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 3000: 0.585450\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 3500: 0.518889\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 4000: 0.444306\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 4500: 0.533362\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5000: 0.425059\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 5500: 0.389614\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 6000: 0.412073\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 6500: 0.388215\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 7000: 0.321419\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 7500: 0.391186\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 8000: 0.371256\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 8500: 0.429245\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.8%\n",
      "Minibatch loss at step 9000: 0.320311\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.6%\n",
      "Test accuracy: 95.5%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 9001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting really good. Let's try one layer deeper with dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 256\n",
    "num_hidden_nodes3 = 128\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(lay1_train, weights2) + biases2)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(lay2_train, weights3) + biases3)\n",
    "  logits = tf.matmul(lay3_train, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "  \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 4000, 0.65, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.411385\n",
      "Minibatch accuracy: 11.7%\n",
      "Validation accuracy: 23.3%\n",
      "Minibatch loss at step 500: 0.478634\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1000: 0.388136\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.9%\n",
      "Minibatch loss at step 1500: 0.271048\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 2000: 0.340876\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 2500: 0.553595\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 3000: 0.365207\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 3500: 0.299175\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 4000: 0.264202\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.9%\n",
      "Minibatch loss at step 4500: 0.336273\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 5000: 0.207242\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 5500: 0.181934\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 6000: 0.207836\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 6500: 0.121624\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 7000: 0.112721\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.6%\n",
      "Minibatch loss at step 7500: 0.136044\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 90.4%\n",
      "Minibatch loss at step 8000: 0.170422\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.5%\n",
      "Minibatch loss at step 8500: 0.133023\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 9000: 0.067770\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 9500: 0.130180\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 10000: 0.141014\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.1%\n",
      "Minibatch loss at step 10500: 0.158475\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 90.7%\n",
      "Minibatch loss at step 11000: 0.064740\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 11500: 0.108055\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 90.9%\n",
      "Minibatch loss at step 12000: 0.083881\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.0%\n",
      "Minibatch loss at step 12500: 0.083453\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 13000: 0.065047\n",
      "Minibatch accuracy: 98.4%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 13500: 0.108142\n",
      "Minibatch accuracy: 94.5%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 14000: 0.127298\n",
      "Minibatch accuracy: 96.9%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 14500: 0.082362\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 15000: 0.074548\n",
      "Minibatch accuracy: 97.7%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 15500: 0.084715\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.3%\n",
      "Minibatch loss at step 16000: 0.097971\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 16500: 0.093715\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.2%\n",
      "Minibatch loss at step 17000: 0.099798\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 91.4%\n",
      "Minibatch loss at step 17500: 0.037153\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.5%\n",
      "Minibatch loss at step 18000: 0.032823\n",
      "Minibatch accuracy: 99.2%\n",
      "Validation accuracy: 91.4%\n",
      "Test accuracy: 95.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 18001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huge! That's my best score on this dataset. I have also tried more parameters, but it does not help:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_hidden_nodes1 = 1024\n",
    "num_hidden_nodes2 = 512\n",
    "num_hidden_nodes3 = 256\n",
    "keep_prob = 0.5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  global_step = tf.Variable(0)\n",
    "\n",
    "  # Variables.\n",
    "  weights1 = tf.Variable(\n",
    "    tf.truncated_normal(\n",
    "        [image_size * image_size, num_hidden_nodes1],\n",
    "        stddev=np.sqrt(2.0 / (image_size * image_size)))\n",
    "    )\n",
    "  biases1 = tf.Variable(tf.zeros([num_hidden_nodes1]))\n",
    "  weights2 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes1, num_hidden_nodes2], stddev=np.sqrt(2.0 / num_hidden_nodes1)))\n",
    "  biases2 = tf.Variable(tf.zeros([num_hidden_nodes2]))\n",
    "  weights3 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes2, num_hidden_nodes3], stddev=np.sqrt(2.0 / num_hidden_nodes2)))\n",
    "  biases3 = tf.Variable(tf.zeros([num_hidden_nodes3]))\n",
    "  weights4 = tf.Variable(\n",
    "    tf.truncated_normal([num_hidden_nodes3, num_labels], stddev=np.sqrt(2.0 / num_hidden_nodes3)))\n",
    "  biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  lay1_train = tf.nn.relu(tf.matmul(tf_train_dataset, weights1) + biases1)\n",
    "  drop1 = tf.nn.dropout(lay1_train, 0.5)\n",
    "  lay2_train = tf.nn.relu(tf.matmul(drop1, weights2) + biases2)\n",
    "  drop2 = tf.nn.dropout(lay2_train, 0.5)\n",
    "  lay3_train = tf.nn.relu(tf.matmul(drop2, weights3) + biases3)\n",
    "  drop3 = tf.nn.dropout(lay3_train, 0.5)\n",
    "  logits = tf.matmul(drop3, weights4) + biases4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels))\n",
    "    \n",
    "  # Optimizer.\n",
    "  learning_rate = tf.train.exponential_decay(0.5, global_step, 5000, 0.80, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  lay1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset, weights1) + biases1)\n",
    "  lay2_valid = tf.nn.relu(tf.matmul(lay1_valid, weights2) + biases2)\n",
    "  lay3_valid = tf.nn.relu(tf.matmul(lay2_valid, weights3) + biases3)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(lay3_valid, weights4) + biases4)\n",
    "  lay1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  lay2_test = tf.nn.relu(tf.matmul(lay1_test, weights2) + biases2)\n",
    "  lay3_test = tf.nn.relu(tf.matmul(lay2_test, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(lay3_test, weights4) + biases4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 2.822960\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 14.4%\n",
      "Minibatch loss at step 500: 0.690500\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 1000: 0.519191\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1500: 0.396480\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 2000: 0.456162\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2500: 0.668990\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 3000: 0.540077\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 3500: 0.548606\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 4000: 0.377653\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 4500: 0.572805\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 5000: 0.366095\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 5500: 0.277077\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 6000: 0.399963\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 88.5%\n",
      "Minibatch loss at step 6500: 0.385444\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 88.7%\n",
      "Minibatch loss at step 7000: 0.319769\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 7500: 0.349858\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 8000: 0.301899\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 8500: 0.382118\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 9000: 0.343033\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.1%\n",
      "Minibatch loss at step 9500: 0.503128\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10000: 0.456201\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 89.3%\n",
      "Minibatch loss at step 10500: 0.459631\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 89.4%\n",
      "Minibatch loss at step 11000: 0.441240\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 11500: 0.350940\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 12000: 0.484164\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 12500: 0.367846\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 13000: 0.298940\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 13500: 0.323784\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 89.7%\n",
      "Minibatch loss at step 14000: 0.469014\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 14500: 0.423598\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 15000: 0.434770\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 15500: 0.296468\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 16000: 0.359590\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 16500: 0.523981\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 17000: 0.374355\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 17500: 0.276067\n",
      "Minibatch accuracy: 93.0%\n",
      "Validation accuracy: 90.3%\n",
      "Minibatch loss at step 18000: 0.303470\n",
      "Minibatch accuracy: 91.4%\n",
      "Validation accuracy: 90.0%\n",
      "Minibatch loss at step 18500: 0.294294\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.1%\n",
      "Minibatch loss at step 19000: 0.369578\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 19500: 0.337813\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 90.2%\n",
      "Minibatch loss at step 20000: 0.268053\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 90.3%\n",
      "Test accuracy: 95.3%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "tf_gpu",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
