{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Load Data\n",
    "\n",
    "train_input = pd.read_csv('../data/data_train.csv')\n",
    "train_labels = train_input['labels'].values\n",
    "# train_labels = pd.get_dummies(train_input['labels']).as_matrix()\n",
    "del train_input['labels']\n",
    "train_input = train_input.as_matrix()\n",
    "# print(train_input.shape)\n",
    "# print(train_labels.shape)\n",
    "\n",
    "test_input = pd.read_csv(\"../data/data_val.csv\")\n",
    "test_labels = test_input['labels'].values\n",
    "# test_labels = pd.get_dummies(test_input['labels']).as_matrix()\n",
    "del test_input['labels']\n",
    "test_input = test_input.as_matrix()\n",
    "# print(test_input.shape)\n",
    "# print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_height = 28\n",
    "img_width = 28\n",
    "img_channels = 1\n",
    "\n",
    "n_classes = 10\n",
    "batch_size = 50\n",
    "epochs = 2\n",
    "\n",
    "device_name='eren_'\n",
    "model_name = device_name+'pytorch_conv_1'\n",
    "vector_dimension = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 32372.828125\n",
      "1 137864.203125\n",
      "2 5948.13330078125\n",
      "3 4740.07421875\n",
      "4 2728.708251953125\n"
     ]
    }
   ],
   "source": [
    "train_input_, train_labels_ = torch.FloatTensor(train_input), torch.FloatTensor(train_labels)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "          torch.nn.Linear(784, 20),\n",
    "          torch.nn.ReLU(),\n",
    "          torch.nn.Linear(20, 1),\n",
    "        )\n",
    "param = list(model.parameters())\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "optimizer = optim.Adadelta(param)\n",
    "batch_size = 50\n",
    "learning_rate = 1e-4\n",
    "for t in range(5):\n",
    "    x, y = train_input_[t*batch_size:(t+1)*batch_size], train_labels_[t*batch_size:(t+1)*batch_size]\n",
    "\n",
    "    if type(x) != torch.autograd.variable.Variable:\n",
    "        x, y = Variable(x), Variable(y)\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 13.709769248962402\n",
      "4 13.225839614868164\n",
      "4 8.32754898071289\n",
      "4 4.868668556213379\n",
      "4 3.2344887256622314\n",
      "4 2.2833399772644043\n",
      "4 1.1323071718215942\n",
      "4 1.2553083896636963\n",
      "4 1.2968429327011108\n",
      "4 0.8567118048667908\n",
      "4 0.9646856784820557\n",
      "4 0.26497364044189453\n",
      "4 0.38566550612449646\n",
      "4 0.6813852787017822\n",
      "4 0.3424835503101349\n",
      "4 0.41514697670936584\n",
      "4 0.29375848174095154\n",
      "4 0.1549941599369049\n",
      "4 0.524043083190918\n",
      "4 0.3139924705028534\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "## Model Definition for Classification\n",
    "train_input_, train_labels_ = torch.FloatTensor(train_input), torch.LongTensor(train_labels)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 10, 1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.linear = nn.Linear(13*13*10,10)\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous()\n",
    "        x = x.view(-1, img_channels, img_height, img_width)\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "        x = x.view(-1, 10*13*13)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "# print(str(net))\n",
    "\n",
    "param = list(net.parameters())\n",
    "optimizer = optim.Adadelta(param)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "batch_size = 50\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # number of batches\n",
    "    imax = int(train_input.shape[0]/batch_size)\n",
    "    for i in range(10):\n",
    "        # get the inputs\n",
    "        x, y = train_input_[i*batch_size:(i+1)*batch_size], train_labels_[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        if type(x) != torch.autograd.variable.Variable:\n",
    "            x, y = Variable(x), Variable(y)\n",
    "        y_pred = net(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "        net.zero_grad()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 3.1925160884857178\n",
      "4 4.503812789916992\n",
      "4 2.301778554916382\n",
      "4 2.3028433322906494\n",
      "4 2.302583694458008\n",
      "4 2.302583694458008\n",
      "4 2.302583694458008\n",
      "4 2.302583694458008\n",
      "4 2.302583694458008\n",
      "4 2.302583694458008\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "## Model Definition for Classification\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3)\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3)\n",
    "        self.conv4 = nn.Conv2d(64, 64, 3)\n",
    "        self.conv5 = nn.Conv2d(64, 128, 2)\n",
    "        self.conv6 = nn.Conv2d(128, 10, 1)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x.contiguous()\n",
    "        x = x.view(-1, img_channels, img_height, img_width)\n",
    "#         print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool1(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.pool1(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.pool1(x)\n",
    "#         print(x.shape)\n",
    "\n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = x.view(-1,10)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "# print(str(net))\n",
    "\n",
    "param = list(net.parameters())\n",
    "optimizer = optim.Adadelta(param)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # number of batches\n",
    "    imax = int(train_input.shape[0]/batch_size)\n",
    "    for i in range(10):\n",
    "        # get the inputs\n",
    "        x, y = train_input_[i*batch_size:(i+1)*batch_size], train_labels_[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        if type(x) != torch.autograd.variable.Variable:\n",
    "            x, y = Variable(x), Variable(y)\n",
    "        y_pred = net(x)\n",
    "\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        print(t, loss.data[0])\n",
    "\n",
    "        net.zero_grad()\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.data[0]\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
